## üõ†Ô∏è –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞

–ü–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–∞–±–æ—Ç—ã —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ –∑–∞–ø—É—â–µ–Ω–∞ –Ω–∞ –≤–∞—à–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ.

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama
–°–∫–∞—á–∞–π—Ç–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –≤–∞—à–µ–π –û–° :
```bash
# –ù–∞ macOS (—á–µ—Ä–µ–∑ Homebrew):
brew install ollama

# –ù–∞ Linux (–∏—Å–ø–æ–ª—å–∑—É—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏):
curl -sS https://ollama.ai/install.sh | bash
```

### –ó–∞–ø—É—Å–∫ Ollama
–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama —Å–µ—Ä–≤–µ—Ä:
```bash
ollama serve
```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python-–±–∏–±–ª–∏–æ—Ç–µ–∫–∏
–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é Python-–±–∏–±–ª–∏–æ—Ç–µ–∫—É Ollama :
```bash
pip install ollama
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å :
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, llama3.2)
ollama pull llama3.2:1b
```

## üìü –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã —Ä–∞–±–æ—Ç—ã —Å Ollama –∏–∑ Python

### –°–ø–æ—Å–æ–± 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π Python-–±–∏–±–ª–∏–æ—Ç–µ–∫–∏

#### –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
```python
import ollama

# –ë–∞–∑–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
response = ollama.generate(model='llama3.2:1b', prompt='–ü–æ—á–µ–º—É –Ω–µ–±–æ —Å–∏–Ω–µ–µ?')
print(response['response'])
```

#### –ß–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Å–æ–æ–±—â–µ–Ω–∏–π
```python
import ollama

response = ollama.chat(
    model='llama3.2:1b',
    messages=[
        {'role': 'system', 'content': '–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫.'},
        {'role': 'user', 'content': '–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é Python –¥–ª—è –ø–µ—Ä–µ–≤–æ—Ä–æ—Ç–∞ —Å—Ç—Ä–æ–∫–∏.'}
    ]
)
print(response['message']['content'])
```

#### –ü–æ—Ç–æ–∫–æ–≤–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –æ—Ç–≤–µ—Ç–æ–≤
```python
import ollama

# –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
stream = ollama.generate(
    model='llama3.2:1b', 
    prompt='–û–ø–∏—à–∏ —Ä–µ–∫—É—Ä—Å–∏—é –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏.',
    stream=True
)

for chunk in stream:
    print(chunk['response'], end='', flush=True)

# –ü–æ—Ç–æ–∫–æ–≤—ã–π —á–∞—Ç
stream = ollama.chat(
    model='llama3.2:1b',
    messages=[
        {'role': 'user', 'content': '–ù–∞–ø–∏—à–∏ —Ö–∞–π–∫—É –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏.'}
    ],
    stream=True
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

### –°–ø–æ—Å–æ–± 2: –ü—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ REST API

–ï—Å–ª–∏ –≤—ã –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç–µ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å HTTP-–∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å HTTP :

#### –ß–∞—Ç —á–µ—Ä–µ–∑ API
```python
import requests
import json

url = "http://localhost:11434/api/chat"

payload = {
    "model": "llama3.2:1b",
    "messages": [
        {"role": "system", "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ Python."},
        {"role": "user", "content": "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫—É."}
    ]
}

response = requests.post(url, json=payload, stream=True)

for line in response.iter_lines():
    if line:
        data = json.loads(line)
        content = data.get("message", {}).get("content", "")
        print(content, end="")
```

#### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ API
```python
import requests

url = "http://localhost:11434/api/generate"
payload = {
    "model": "llama3.2:1b",
    "prompt": "–û–±—ä—è—Å–Ω–∏ —Ä–µ–∫—É—Ä—Å–∏—é –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏."
}

response = requests.post(url, json=payload, stream=True)
for line in response.iter_lines():
    if line:
        print(line.decode("utf-8"))
```

## üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

### –†–∞–±–æ—Ç–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

Ollama –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ :

```python
import ollama

response = ollama.generate(
    model='llama3.2:1b',
    prompt='–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞—Ö Python –¥–ª—è data science',
    options={
        'temperature': 0.7,      # –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ (0-1)
        'top_p': 0.9,           # –ù—É–∫–ª–µ—É—Å-—Å—ç–º–ø–ª–∏–Ω–≥
        'top_k': 40,            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        'num_predict': 256,     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        'stop': ['\n', '###'],  # –°—Ç—Ä–æ–∫–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        'repeat_penalty': 1.1,  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        'seed': 42              # Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    }
)
```

### –ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞

–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞—Ç-–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö :

```python
import ollama

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
messages = []

while True:
    user_input = input('–í—ã: ')
    if user_input.lower() == '–≤—ã—Ö–æ–¥':
        break
        
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
    messages.append({'role': 'user', 'content': user_input})
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏
    response = ollama.chat(
        model='llama3.2:1b',
        messages=messages
    )
    
    answer = response['message']['content']
    print(f'–ë–æ—Ç: {answer}')
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
    messages.append({'role': 'assistant', 'content': answer})
```

### –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ Ollama

–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –∫–ª–∏–µ–Ω—Ç–∞ :

```python
import requests
import json
from typing import Dict, Any, List, Optional

class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.context = None
    
    def generate(self, 
                prompt: str, 
                model: str = "llama3.2:1b",
                temperature: float = 0.7,
                top_p: float = 0.9,
                num_predict: int = 256,
                **kwargs) -> Dict[str, Any]:
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "top_p": top_p,
                "num_predict": num_predict,
                **kwargs
            }
        }
        
        if self.context:
            payload["context"] = self.context
            
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            if "context" in result:
                self.context = result["context"]
                
            return result
            
        except requests.exceptions.RequestException as e:
            return {"error": str(e)}
    
    def clear_context(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        self.context = None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
client = OllamaClient()
result = client.generate("–†–∞—Å—Å–∫–∞–∂–∏ –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏")
print(result.get("response", "–û—à–∏–±–∫–∞"))
```

## üß† –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏

### –†–∞–±–æ—Ç–∞ —Å–æ "–º—ã—Å–ª—è—â–∏–º–∏" –º–æ–¥–µ–ª—è–º–∏

–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, qwen3) –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –≤—ã–≤–æ–¥ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π :

```python
import ollama
import re

response = ollama.chat(
    model="qwen3",
    messages=[
        {"role": "user", "content": "–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –ê–≤—Å—Ç—Ä–∞–ª–∏–∏?"}
    ]
)

content = response['message']['content']

# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è
thinking = re.findall(r"<think>(.*?)</think>", content, re.DOTALL)
answer = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL)

print("üß† –ü—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è:\n", thinking[0].strip() if thinking else "N/A")
print("\n‚úÖ –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:\n", answer.strip())
```

### –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥

–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON :

```python
import ollama
import json

response = ollama.chat(
    model='llama3.2:1b',
    messages=[{
        'role': 'user', 
        'content': '–û–ø–∏—à–∏ —Å—Ç–æ–ª–∏—Ü—É –°–®–ê –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å –ø–æ–ª—è–º–∏: –Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–∞—Å–µ–ª–µ–Ω–∏–µ, –ø–ª–æ—â–∞–¥—å.'
    }],
    format='json',  # –£–∫–∞–∑—ã–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
    options={'temperature': 0}  # –î–ª—è –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
)

try:
    json_response = json.loads(response['message']['content'])
    print(json.dumps(json_response, indent=2, ensure_ascii=False))
except json.JSONDecodeError:
    print("–û—Ç–≤–µ—Ç –Ω–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:", response['message']['content'])
```

### –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞

–î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ :

```python
import asyncio
from ollama import AsyncClient

async def async_chat_example():
    client = AsyncClient()
    
    messages = [
        {'role': 'user', 'content': '–ü–æ—á–µ–º—É —Ç—Ä–∞–≤–∞ –∑–µ–ª–µ–Ω–∞—è?'}
    ]
    
    # –û–±—ã—á–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    response = await client.chat(model='llama3.2:1b', messages=messages)
    print(response['message']['content'])
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ—Ç–æ–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    async for part in await client.chat(model='llama3.2:1b', messages=messages, stream=True):
        print(part['message']['content'], end='', flush=True)

# –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
asyncio.run(async_chat_example())
```

## üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ß–∞—Ç-–±–æ—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

```python
import ollama

class ChatBot:
    def __init__(self, model: str = "llama3.2:1b"):
        self.model = model
        self.conversation_history = []
        
    def add_system_prompt(self, prompt: str):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
        self.conversation_history.append({
            'role': 'system', 
            'content': prompt
        })
    
    def chat(self, user_input: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"""
        self.conversation_history.append({
            'role': 'user', 
            'content': user_input
        })
        
        response = ollama.chat(
            model=self.model,
            messages=self.conversation_history
        )
        
        answer = response['message']['content']
        
        self.conversation_history.append({
            'role': 'assistant', 
            'content': answer
        })
        
        return answer
    
    def clear_history(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        self.conversation_history = []

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
bot = ChatBot()
bot.add_system_prompt("–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.")

while True:
    user_input = input("–í—ã: ")
    if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
        break
        
    response = bot.chat(user_input)
    print(f"–ë–æ—Ç: {response}")
```

### –ü—Ä–∏–º–µ—Ä 2: –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

```python
import ollama

class WorkflowAutomation:
    def __init__(self, model: str = "llama3.2:1b"):
        self.model = model
    
    def summarize_text(self, text: str, max_length: int = 200) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"""
        prompt = f"""
        –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –≤ {max_length} —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ –º–µ–Ω—å—à–µ:
        
        {text}
        """
        
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={'temperature': 0.3, 'num_predict': max_length}
        )
        
        return response['response']
    
    def generate_code(self, description: str, language: str = "python") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é"""
        prompt = f"""
        –ù–∞–ø–∏—à–∏ –∫–æ–¥ –Ω–∞ {language} –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏:
        {description}
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –î–æ–±–∞–≤—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        - –°–ª–µ–¥—É–π best practices
        - –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –∫–æ–¥, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π
        """
        
        response = ollama.generate(
            model=self.model,
            prompt=prompt,
            options={'temperature': 0.2}
        )
        
        return response['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
automator = WorkflowAutomation()

# –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
text = """
–û–ª–ª–æ–º–∞ - —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ª–æ–∫–∞–ª—å–Ω–æ...
"""
summary = automator.summarize_text(text)
print("–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è:", summary)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞
code = automator.generate_code("—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª–∞")
print("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥:")
print(code)
```

## üöÄ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ Ollama —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ :

```python
import ollama

# –°–ø–∏—Å–æ–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
models = ollama.list()
print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
for model in models['models']:
    print(f" - {model['name']}")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
model_info = ollama.show('llama3.2:1b')
print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:", model_info)

# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
modelfile = '''
FROM llama3.2:1b
SYSTEM –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ Python –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.
'''

ollama.create(model='my-python-assistant', modelfile=modelfile)

# –£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ollama.delete('my-python-assistant')
```

## üîç –û—Ç–ª–∞–¥–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
import ollama
from requests.exceptions import ConnectionError

def safe_ollama_chat(messages, model='llama3.2:1b', max_retries=3):
    for attempt in range(max_retries):
        try:
            response = ollama.chat(model=model, messages=messages)
            return response['message']['content']
        
        except ConnectionError:
            if attempt < max_retries - 1:
                print(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2} –∏–∑ {max_retries}...")
                continue
            else:
                return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞."
        
        except Exception as e:
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
result = safe_ollama_chat([{'role': 'user', 'content': '–ü—Ä–∏–≤–µ—Ç!'}])
print(result)
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞

```python
import requests

def check_ollama_health():
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        return response.status_code == 200
    except:
        return False

if check_ollama_health():
    print("Ollama —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ")
else:
    print("Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
```

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

| –ó–∞–¥–∞—á–∞ | REST API | –ö–ª–∏–µ–Ω—Ç Python |
|--------|----------|---------------|
| –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ | `/api/generate` | `ollama.generate()` |
| –î–∏–∞–ª–æ–≥–æ–≤—ã–π —á–∞—Ç | `/api/chat` | `ollama.chat()` |
| –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–∏ | –î–∞ | –î–∞ |
| –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—è–º–∏ | –ß–µ—Ä–µ–∑ CLI | `ollama.list()`, `ollama.pull()` –∏ –¥—Ä. |
| –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º | –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ | –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —á–µ—Ä–µ–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫—É |
| –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ | –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ | `AsyncClient` |
