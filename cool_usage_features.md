## üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏

### 1. **–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (Model Ensembling)**

```python
import ollama
import asyncio
from typing import List, Dict
import numpy as np

class ModelEnsemble:
    def __init__(self):
        self.models = [
            'llama3.1:8b',      # –î–ª—è –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π
            'qwen2.5:7b',       # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞  
            'deepseek-r1:8b',   # –î–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
            'codellama:7b'      # –î–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        ]
    
    async def get_ensemble_response(self, prompt: str, strategy: str = "vote") -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        responses = []
        
        for model in self.models:
            try:
                response = ollama.generate(model=model, prompt=prompt)
                responses.append({
                    'model': model,
                    'response': response['response'],
                    'confidence': self.calculate_confidence(response['response'])
                })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {model}: {e}")
        
        if strategy == "vote":
            return self.majority_vote(responses)
        elif strategy == "confidence":
            return self.highest_confidence(responses)
        elif strategy == "combined":
            return self.combine_responses(responses)
    
    def calculate_confidence(self, response: str) -> float:
        """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞"""
        confidence_indicators = [
            len(response) > 50,                    # –î–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –æ–±—ã—á–Ω–æ —É–≤–µ—Ä–µ–Ω–Ω–µ–µ
            '?' not in response[-10:],             # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –∫–æ–Ω—Ü–µ
            not any(word in response.lower() for word in ['–Ω–µ –∑–Ω–∞—é', '–≤–æ–∑–º–æ–∂–Ω–æ', '–Ω–∞–≤–µ—Ä–Ω–æ–µ'])
        ]
        return sum(confidence_indicators) / len(confidence_indicators)
    
    def majority_vote(self, responses: List[Dict]) -> str:
        """–í—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
        return max(responses, key=lambda x: x['confidence'])['response']
    
    def highest_confidence(self, responses: List[Dict]) -> str:
        """–í—ã–±–æ—Ä –æ—Ç–≤–µ—Ç–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é"""
        return max(responses, key=lambda x: x['confidence'])['response']
    
    def combine_responses(self, responses: List[Dict]) -> str:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–∏—Ö —á–∞—Å—Ç–µ–π –≤—Å–µ—Ö –æ—Ç–≤–µ—Ç–æ–≤"""
        combined = "–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π:\n\n"
        for i, resp in enumerate(sorted(responses, key=lambda x: x['confidence'], reverse=True), 1):
            combined += f"–ú–æ–¥–µ–ª—å {i} ({resp['model'].split(':')[0]}): {resp['response']}\n\n"
        return combined

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
ensemble = ModelEnsemble()
response = asyncio.run(ensemble.get_ensemble_response("–û–±—ä—è—Å–Ω–∏ –∫–≤–∞–Ω—Ç–æ–≤—É—é –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç—å"))
print(response)
```

### 2. **–¶–µ–ø–æ—á–∫–∏ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π (Chain of Thought)**

```python
class AdvancedReasoning:
    def __init__(self, model: str = "deepseek-r1:8b"):
        self.model = model
    
    def complex_reasoning(self, problem: str) -> str:
        """–†–µ—à–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ —Ü–µ–ø–æ—á–∫—É —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π"""
        
        # –®–∞–≥ 1: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã
        analysis_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é –ø—Ä–æ–±–ª–µ–º—É –∏ —Ä–∞–∑–±–µ–π –µ—ë –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏:
        {problem}
        
        –í—ã–≤–µ–¥–∏ —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –ø–æ–¥–∑–∞–¥–∞—á –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
        """
        
        analysis = ollama.generate(
            model=self.model,
            prompt=analysis_prompt,
            options={'temperature': 0.3}
        )
        
        # –®–∞–≥ 2: –†–µ—à–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –ø–æ–¥–∑–∞–¥–∞—á–∏
        solution_prompt = f"""
        –ü—Ä–æ–±–ª–µ–º–∞: {problem}
        
        –ü–æ–¥–∑–∞–¥–∞—á–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è:
        {analysis['response']}
        
        –†–µ—à–∏ –∫–∞–∂–¥—É—é –ø–æ–¥–∑–∞–¥–∞–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø–æ–∫–∞–∑—ã–≤–∞—è —Ö–æ–¥ –º—ã—Å–ª–µ–π.
        """
        
        solution = ollama.generate(
            model=self.model,
            prompt=solution_prompt,
            options={'temperature': 0.7}
        )
        
        # –®–∞–≥ 3: –°–∏–Ω—Ç–µ–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        final_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ–¥–∑–∞–¥–∞—á —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç:
        
        –ò—Å—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞: {problem}
        –†–µ—à–µ–Ω–∏–µ –ø–æ–¥–∑–∞–¥–∞—á: {solution['response']}
        
        –î–∞–π –∫—Ä–∞—Ç–∫–∏–π, –Ω–æ –ø–æ–ª–Ω—ã–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç.
        """
        
        final = ollama.generate(
            model=self.model,
            prompt=final_prompt,
            options={'temperature': 0.5}
        )
        
        return final['response']
    
    def socratic_dialogue(self, question: str, max_turns: int = 5) -> str:
        """–°–æ–∫—Ä–∞—Ç–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ª–æ–≥ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"""
        conversation = [{'role': 'user', 'content': question}]
        
        for turn in range(max_turns):
            # –ú–æ–¥–µ–ª—å –∑–∞–¥–∞–µ—Ç —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            clarification = ollama.chat(
                model=self.model,
                messages=conversation + [{
                    'role': 'system', 
                    'content': '–ó–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å —á—Ç–æ–±—ã –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –ø—Ä–æ–±–ª–µ–º—É. –ù–µ –¥–∞–≤–∞–π –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É.'
                }]
            )
            
            clarification_question = clarification['message']['content']
            print(f"ü§î –£—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å: {clarification_question}")
            
            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ - –ø–æ–ª—É—á–∞–µ–º –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
            user_answer = input("–í–∞—à –æ—Ç–≤–µ—Ç: ")
            
            conversation.extend([
                {'role': 'assistant', 'content': clarification_question},
                {'role': 'user', 'content': user_answer}
            ])
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_response = ollama.chat(
            model=self.model,
            messages=conversation + [{
                'role': 'system', 
                'content': '–¢–µ–ø–µ—Ä—å –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–π –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'
            }]
        )
        
        return final_response['message']['content']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
reasoner = AdvancedReasoning()
result = reasoner.complex_reasoning("–ö–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã –≤ —Å—Ç–∞—Ä—Ç–∞–ø–µ?")
print(result)
```

### 3. **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**

```python
class SmartModelRouter:
    def __init__(self):
        self.model_map = {
            'coding': ['codellama:7b', 'deepseek-coder:6.7b'],
            'creative': ['llama3.1:8b', 'qwen2.5:7b'],
            'reasoning': ['deepseek-r1:8b', 'llama3.1:8b'],
            'russian': ['qwen2.5:7b', 'saiga:7b'],
            'math': ['wizardmath:7b', 'mathstral:7b'],
            'general': ['llama3.1:8b', 'qwen2.5:7b']
        }
    
    def detect_intent(self, message: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        intent_prompt = f"""
        –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –í–∞—Ä–∏–∞–Ω—Ç—ã: coding, creative, reasoning, russian, math, general.
        
        –°–æ–æ–±—â–µ–Ω–∏–µ: {message}
        
        –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ - —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞.
        """
        
        response = ollama.generate(
            model='llama3.1:8b',
            prompt=intent_prompt,
            options={'temperature': 0.1}
        )
        
        intent = response['response'].strip().lower()
        return intent if intent in self.model_map else 'general'
    
    def get_best_model(self, intent: str, available_models: List[str] = None) -> str:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–¥–∞—á–∏"""
        if available_models is None:
            available_models = self.get_available_models()
        
        candidates = [model for model in self.model_map[intent] if model in available_models]
        return candidates[0] if candidates else available_models[0]
    
    def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            models = ollama.list()
            return [model['name'] for model in models['models']]
        except:
            return ['llama3.1:8b']  # fallback
    
    def route_request(self, message: str) -> str:
        """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        intent = self.detect_intent(message)
        best_model = self.get_best_model(intent)
        
        print(f"üéØ –û–ø—Ä–µ–¥–µ–ª–µ–Ω –∏–Ω—Ç–µ–Ω—Ç: {intent}, –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {best_model}")
        
        response = ollama.generate(model=best_model, prompt=message)
        return f"ü§ñ [{best_model}]: {response['response']}"

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
router = SmartModelRouter()
response = router.route_request("–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é Python –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏")
print(response)
```

## üîß –ö—Ä—É—Ç—ã–µ –∏–¥–µ–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤

### 4. **AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º**

```python
class AITutor:
    def __init__(self, subject: str = "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"):
        self.subject = subject
        self.student_level = "beginner"
        self.learning_style = "–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π"
        self.progress = []
    
    def assess_student_level(self, student_response: str) -> str:
        """–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —Å—Ç—É–¥–µ–Ω—Ç–∞"""
        assessment_prompt = f"""
        –û—Ü–µ–Ω–∏ —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ –æ—Ç–≤–µ—Ç—É:
        
        –û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: {student_response}
        –ü—Ä–µ–¥–º–µ—Ç: {self.subject}
        
        –í–∞—Ä–∏–∞–Ω—Ç—ã —É—Ä–æ–≤–Ω–µ–π: beginner, intermediate, advanced.
        –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —É—Ä–æ–≤–µ–Ω—å.
        """
        
        response = ollama.generate(
            model='deepseek-r1:8b',
            prompt=assessment_prompt,
            options={'temperature': 0.1}
        )
        
        self.student_level = response['response'].strip().lower()
        return self.student_level
    
    def generate_lesson(self, topic: str) -> dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É—Ä–æ–∫–∞"""
        lesson_prompt = f"""
        –°–æ–∑–¥–∞–π —É—Ä–æ–∫ –ø–æ —Ç–µ–º–µ: {topic}
        –£—Ä–æ–≤–µ–Ω—å —Å—Ç—É–¥–µ–Ω—Ç–∞: {self.student_level}
        –°—Ç–∏–ª—å –æ–±—É—á–µ–Ω–∏—è: {self.learning_style}
        –ü—Ä–µ–¥–º–µ—Ç: {self.subject}
        
        –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
        1. –ö—Ä–∞—Ç–∫–∞—è —Ç–µ–æ—Ä–∏—è
        2. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä
        3. –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è
        4. –í–æ–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è
        
        –í–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
        """
        
        response = ollama.generate(
            model='qwen2.5:7b',
            prompt=lesson_prompt,
            options={'temperature': 0.7}
        )
        
        # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
        try:
            import json
            lesson_data = json.loads(response['response'])
            return lesson_data
        except:
            return {
                'theory': response['response'],
                'example': '–ü—Ä–∏–º–µ—Ä –±—É–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º —É—Ä–æ–∫–µ',
                'exercise': '–ü–æ–ø—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ—Å—å —Å –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Ç–µ–æ—Ä–∏–µ–π',
                'question': '–ß—Ç–æ –≤—ã –ø–æ–Ω—è–ª–∏ –∏–∑ —ç—Ç–æ–≥–æ —É—Ä–æ–∫–∞?'
            }
    
    def provide_feedback(self, student_answer: str, correct_answer: str) -> str:
        """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        feedback_prompt = f"""
        –î–∞–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å —Å—Ç—É–¥–µ–Ω—Ç—É:
        
        –û—Ç–≤–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞: {student_answer}
        –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}
        –£—Ä–æ–≤–µ–Ω—å —Å—Ç—É–¥–µ–Ω—Ç–∞: {self.student_level}
        
        –ë—É–¥—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º, —É–∫–∞–∂–∏ –Ω–∞ –æ—à–∏–±–∫–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è.
        """
        
        response = ollama.generate(
            model='deepseek-r1:8b',
            prompt=feedback_prompt,
            options={'temperature': 0.8}
        )
        
        return response['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
tutor = AITutor("Python")
lesson = tutor.generate_lesson("—Ñ—É–Ω–∫—Ü–∏–∏ –≤ Python")
print(f"üìö –¢–µ–æ—Ä–∏—è: {lesson['theory']}")
print(f"üí° –ü—Ä–∏–º–µ—Ä: {lesson['example']}")
```

### 5. **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–¥–∞**

```python
class CodeAnalyzer:
    def __init__(self):
        self.metrics = {}
    
    def analyze_code_quality(self, code: str, language: str = "python") -> dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞"""
        analysis_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–¥ –Ω–∞ {language} –∏ –æ—Ü–µ–Ω–∏:
        
        –ö–æ–¥:
        ```{language}
        {code}
        ```
        
        –û—Ü–µ–Ω–∏ –ø–æ —à–∫–∞–ª–µ 1-10:
        - –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å
        - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å  
        - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ best practices
        - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
        - –ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å
        
        –£–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è.
        –í–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
        """
        
        response = ollama.generate(
            model='codellama:7b',
            prompt=analysis_prompt,
            options={'temperature': 0.3}
        )
        
        try:
            import json
            return json.loads(response['response'])
        except:
            return {'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥'}
    
    def suggest_optimizations(self, code: str, language: str = "python") -> list:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π"""
        optimization_prompt = f"""
        –ü—Ä–µ–¥–ª–æ–∂–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–æ–¥–∞:
        
        ```{language}
        {code}
        ```
        
        –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞:
        - –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏
        - –£–ª—É—á—à–µ–Ω–∏–∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞—Ö
        
        –í–µ—Ä–Ω–∏ —Å–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
        """
        
        response = ollama.generate(
            model='deepseek-coder:6.7b',
            prompt=optimization_prompt,
            options={'temperature': 0.5}
        )
        
        return [suggestion.strip() for suggestion in response['response'].split('\n') if suggestion.strip()]
    
    def generate_tests(self, code: str, language: str = "python") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è unit-—Ç–µ—Å—Ç–æ–≤"""
        test_prompt = f"""
        –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π unit-—Ç–µ—Å—Ç—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–¥–∞:
        
        ```{language}
        {code}
        ```
        
        –í–µ—Ä–Ω–∏ –≥–æ—Ç–æ–≤—ã–π –∫–æ–¥ —Ç–µ—Å—Ç–æ–≤ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏.
        """
        
        response = ollama.generate(
            model='codellama:7b',
            prompt=test_prompt,
            options={'temperature': 0.4}
        )
        
        return response['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
analyzer = CodeAnalyzer()
code = """
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
"""

quality_report = analyzer.analyze_code_quality(code)
optimizations = analyzer.suggest_optimizations(code)
tests = analyzer.generate_tests(code)

print("üìä –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ:", quality_report)
print("‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:", optimizations)
print("üß™ –¢–µ—Å—Ç—ã:", tests)
```

### 6. **–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä**

```python
class ContentGenerator:
    def __init__(self, brand_voice: str = "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π", tone: str = "–¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π"):
        self.brand_voice = brand_voice
        self.tone = tone
        self.content_history = []
    
    def analyze_audience(self, audience_description: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"""
        audience_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ü–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é: {audience_description}
        
        –û–ø—Ä–µ–¥–µ–ª–∏:
        - –ë–æ–ª–∏ –∏ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏
        - –Ø–∑—ã–∫ –æ–±—â–µ–Ω–∏—è
        - –ò–Ω—Ç–µ—Ä–µ—Å—ã
        - –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π
        
        –í–µ—Ä–Ω–∏ –∞–Ω–∞–ª–∏–∑ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ.
        """
        
        response = ollama.generate(
            model='qwen2.5:7b',
            prompt=audience_prompt,
            options={'temperature': 0.6}
        )
        
        return {'audience_analysis': response['response']}
    
    def generate_content_strategy(self, topic: str, platform: str, audience: dict) -> list:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        strategy_prompt = f"""
        –°–æ–∑–¥–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è:
        –¢–µ–º–∞: {topic}
        –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: {platform}
        –ê—É–¥–∏—Ç–æ—Ä–∏—è: {audience['audience_analysis']}
        –ì–æ–ª–æ—Å –±—Ä–µ–Ω–¥–∞: {self.brand_voice}
        –¢–æ–Ω: {self.tone}
        
        –ü—Ä–µ–¥–ª–æ–∂–∏ 5 –∏–¥–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º.
        """
        
        response = ollama.generate(
            model='llama3.1:8b',
            prompt=strategy_prompt,
            options={'temperature': 0.8}
        )
        
        ideas = [idea.strip() for idea in response['response'].split('\n') if idea.strip()]
        return ideas[:5]
    
    def create_content_piece(self, idea: str, word_count: int = 500) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        content_prompt = f"""
        –ù–∞–ø–∏—à–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–¥–µ–∏: {idea}
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - –ì–æ–ª–æ—Å –±—Ä–µ–Ω–¥–∞: {self.brand_voice}
        - –¢–æ–Ω: {self.tone}
        - –û–±—ä–µ–º: {word_count} —Å–ª–æ–≤
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        - –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
        
        –°–æ–∑–¥–∞–π –≥–æ—Ç–æ–≤—ã–π –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª.
        """
        
        response = ollama.generate(
            model='qwen2.5:7b',
            prompt=content_prompt,
            options={'temperature': 0.7, 'num_predict': 1000}
        )
        
        self.content_history.append({
            'idea': idea,
            'content': response['response'],
            'timestamp': '2024-01-01'  # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å datetime
        })
        
        return response['response']
    
    def a_b_test_content(self, content_a: str, content_b: str, metric: str = "engagement") -> dict:
        """A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        test_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –º–µ—Ç—Ä–∏–∫–µ: {metric}
        
        –ö–æ–Ω—Ç–µ–Ω—Ç A:
        {content_a}
        
        –ö–æ–Ω—Ç–µ–Ω—Ç B:
        {content_b}
        
        –ü—Ä–µ–¥—Å–∫–∞–∂–∏, –∫–∞–∫–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–∫–∞–∂–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø–æ—á–µ–º—É.
        """
        
        response = ollama.generate(
            model='deepseek-r1:8b',
            prompt=test_prompt,
            options={'temperature': 0.5}
        )
        
        return {
            'predicted_winner': 'A' if '–∫–æ–Ω—Ç–µ–Ω—Ç a' in response['response'].lower() else 'B',
            'analysis': response['response']
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
content_gen = ContentGenerator("–∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π", "–≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π")
audience = content_gen.analyze_audience("—Å—Ç–∞—Ä—Ç–∞–ø–µ—Ä—ã –≤ IT, 25-35 –ª–µ—Ç")
strategy = content_gen.generate_content_strategy("–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "LinkedIn", audience)

for i, idea in enumerate(strategy, 1):
    print(f"üéØ –ò–¥–µ—è {i}: {idea}")
    content = content_gen.create_content_piece(idea)
    print(f"üìù –ö–æ–Ω—Ç–µ–Ω—Ç: {content[:200]}...\n")
```

## üéØ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∏ –∫–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è

### 7. **–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ Modelfile**

```python
import subprocess
import tempfile
import os

class ModelCustomizer:
    def __init__(self):
        self.template_modelfile = """
FROM {base_model}

SYSTEM \"\"\"
{system_prompt}
\"\"\"

TEMPLATE \"\"\"{template}\"\"\"

PARAMETER temperature {temperature}
PARAMETER top_p {top_p}
PARAMETER num_predict {max_tokens}
"""
    
    def create_custom_model(self, 
                          model_name: str,
                          base_model: str,
                          system_prompt: str,
                          temperature: float = 0.7,
                          top_p: float = 0.9,
                          max_tokens: int = 2048) -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        template = """{% for message in messages %}
{% if message['role'] == 'user' %}
### –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {{ message['content'] }}
{% elif message['role'] == 'assistant' %}
### –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {{ message['content'] }}
{% endif %}
{% endfor %}"""
        
        modelfile_content = self.template_modelfile.format(
            base_model=base_model,
            system_prompt=system_prompt,
            template=template,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        with tempfile.NamedTemporaryFile(mode='w', suffix='.modelfile', delete=False) as f:
            f.write(modelfile_content)
            temp_path = f.name
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama CLI
            result = subprocess.run([
                'ollama', 'create', model_name, '-f', temp_path
            ], capture_output=True, text=True, check=True)
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
        finally:
            os.unlink(temp_path)
    
    def create_specialized_assistant(self, specialization: str, style: str = "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π") -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        
        specializations = {
            'legal': {
                'base_model': 'llama3.1:8b',
                'system_prompt': f'''–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Ç–æ—á–Ω–æ, —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ.
                –°—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è: {style}. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–µ–Ω –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö.'''
            },
            'medical': {
                'base_model': 'qwen2.5:7b', 
                'system_prompt': f'''–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –î–∞–≤–∞–π –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –Ω–æ –Ω–∞–ø–æ–º–∏–Ω–∞–π –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –≤—Ä–∞—á—É.
                –°—Ç–∏–ª—å: {style}. –ë—É–¥—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω –≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö.'''
            },
            'technical': {
                'base_model': 'codellama:7b',
                'system_prompt': f'''–¢—ã - senior —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –ü–æ–º–æ–≥–∞–π —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, –¥–∞–≤–∞–π –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞.
                –°—Ç–∏–ª—å: {style}. –û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ.'''
            }
        }
        
        if specialization not in specializations:
            raise ValueError(f"–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è {specialization} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
        
        config = specializations[specialization]
        model_name = f"{specialization}-assistant-{style}"
        
        success = self.create_custom_model(
            model_name=model_name,
            base_model=config['base_model'],
            system_prompt=config['system_prompt']
        )
        
        return model_name if success else None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
customizer = ModelCustomizer()
legal_model = customizer.create_specialized_assistant('legal', '—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π')
if legal_model:
    response = ollama.generate(model=legal_model, prompt="–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –û–û–û?")
    print(response['response'])
```

### 8. **–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å few-shot –ø—Ä–∏–º–µ—Ä–∞–º–∏**

```python
class FewShotLearner:
    def __init__(self, base_model: str = "llama3.1:8b"):
        self.base_model = base_model
        self.examples = {}
    
    def add_examples(self, task_type: str, examples: List[dict]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ few-shot –ø—Ä–∏–º–µ—Ä–æ–≤"""
        if task_type not in self.examples:
            self.examples[task_type] = []
        
        self.examples[task_type].extend(examples)
    
    def generate_with_examples(self, task_type: str, new_input: str, max_examples: int = 3) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º few-shot –æ–±—É—á–µ–Ω–∏—è"""
        if task_type not in self.examples or not self.examples[task_type]:
            # –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            return ollama.generate(model=self.base_model, prompt=new_input)['response']
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        import random
        selected_examples = random.sample(self.examples[task_type], 
                                        min(max_examples, len(self.examples[task_type])))
        
        # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
        prompt = "–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏:\n\n"
        
        for i, example in enumerate(selected_examples, 1):
            prompt += f"–ü—Ä–∏–º–µ—Ä {i}:\n"
            prompt += f"–í—Ö–æ–¥: {example['input']}\n"
            prompt += f"–í—ã—Ö–æ–¥: {example['output']}\n\n"
        
        prompt += f"–¢–µ–ø–µ—Ä—å –≤—ã–ø–æ–ª–Ω–∏ –∑–∞–¥–∞—á—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –≤—Ö–æ–¥–∞:\n"
        prompt += f"–í—Ö–æ–¥: {new_input}\n"
        prompt += f"–í—ã—Ö–æ–¥:"
        
        response = ollama.generate(
            model=self.base_model,
            prompt=prompt,
            options={'temperature': 0.3}
        )
        
        return response['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
learner = FewShotLearner()

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
sentiment_examples = [
    {
        'input': '–≠—Ç–æ—Ç –ø—Ä–æ–¥—É–∫—Ç –ø—Ä–æ—Å—Ç–æ –ø–æ—Ç—Ä—è—Å–∞—é—â–∏–π!',
        'output': 'POSITIVE'
    },
    {
        'input': '–£–∂–∞—Å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–∏–∫–æ–≥–¥–∞ –±–æ–ª—å—à–µ –Ω–µ –∫—É–ø–ª—é',
        'output': 'NEGATIVE'  
    },
    {
        'input': '–ù–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–≤–∞—Ä –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏',
        'output': 'NEUTRAL'
    }
]

learner.add_examples('sentiment', sentiment_examples)

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ
result = learner.generate_with_examples(
    'sentiment', 
    '–ö–∞—á–µ—Å—Ç–≤–æ —Ö–æ—Ä–æ—à–µ–µ, –Ω–æ –¥–æ—Å—Ç–∞–≤–∫–∞ –ø–æ–¥–≤–µ–ª–∞'
)
print(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result}")
```

## üîÆ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

### 9. **–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã**

```python
class MultiAgentSystem:
    def __init__(self):
        self.agents = {}
    
    def create_agent(self, name: str, role: str, expertise: str, model: str = "llama3.1:8b"):
        """–°–æ–∑–¥–∞–Ω–∏–µ AI-–∞–≥–µ–Ω—Ç–∞"""
        self.agents[name] = {
            'role': role,
            'expertise': expertise,
            'model': model,
            'memory': []
        }
    
    def agent_discussion(self, topic: str, participants: List[str]) -> str:
        """–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –¥–∏—Å–∫—É—Å—Å–∏–∏ –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏"""
        discussion = f"–¢–µ–º–∞ –¥–∏—Å–∫—É—Å—Å–∏–∏: {topic}\n\n"
        
        for round_num in range(3):  # 3 —Ä–∞—É–Ω–¥–∞ –¥–∏—Å–∫—É—Å—Å–∏–∏
            discussion += f"--- –†–∞—É–Ω–¥ {round_num + 1} ---\n"
            
            for agent_name in participants:
                if agent_name not in self.agents:
                    continue
                
                agent = self.agents[agent_name]
                
                # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏—Å–∫—É—Å—Å–∏–∏
                prompt = f"""
                –¢—ã - {agent['role']} —Å —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π –≤ {agent['expertise']}.
                
                –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏—Å–∫—É—Å—Å–∏–∏:
                {discussion}
                
                –î–æ–±–∞–≤—å —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –¥–∏—Å–∫—É—Å—Å–∏—é. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Å–≤–æ—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É.
                –¢–≤–æ—è —Ä–æ–ª—å: {agent['role']}
                """
                
                response = ollama.generate(
                    model=agent['model'],
                    prompt=prompt,
                    options={'temperature': 0.8}
                )
                
                agent_contribution = response['response']
                discussion += f"{agent_name} ({agent['role']}): {agent_contribution}\n\n"
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å –∞–≥–µ–Ω—Ç–∞
                agent['memory'].append({
                    'round': round_num + 1,
                    'contribution': agent_contribution
                })
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑
        synthesis_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—É—Å—Å–∏–∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤—ã–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å:
        
        {discussion}
        
        –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        """
        
        final_response = ollama.generate(
            model='deepseek-r1:8b',
            prompt=synthesis_prompt,
            options={'temperature': 0.5}
        )
        
        return final_response['response']

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
system = MultiAgentSystem()

# –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–æ–≤
system.create_agent("tech_lead", "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º", "codellama:7b")
system.create_agent("product_manager", "–ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä", "–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "llama3.1:8b") 
system.create_agent("ux_designer", "UX –¥–∏–∑–∞–π–Ω–µ—Ä", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç", "qwen2.5:7b")

# –ó–∞–ø—É—Å–∫–∞–µ–º –¥–∏—Å–∫—É—Å—Å–∏—é
result = system.agent_discussion(
    "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ–≥–æ –º–æ–±–∏–ª—å–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –±–∞–Ω–∫–∏–Ω–≥–∞",
    ["tech_lead", "product_manager", "ux_designer"]
)

print("ü§ù –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏—Å–∫—É—Å—Å–∏–∏:")
print(result)
```

### 10. **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤**

```python
class QualityEvaluator:
    def __init__(self):
        self.metrics = ['relevance', 'accuracy', 'completeness', 'clarity']
    
    def evaluate_response(self, question: str, response: str, context: str = "") -> dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –º–µ—Ç—Ä–∏–∫–∞–º"""
        
        evaluation_prompt = f"""
        –û—Ü–µ–Ω–∏ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –º–µ—Ç—Ä–∏–∫–∞–º (1-10):
        
        –í–æ–ø—Ä–æ—Å: {question}
        –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
        –û—Ç–≤–µ—Ç: {response}
        
        –ú–µ—Ç—Ä–∏–∫–∏:
        - Relevance (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –≤–æ–ø—Ä–æ—Å—É)
        - Accuracy (—Ç–æ—á–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏) 
        - Completeness (–ø–æ–ª–Ω–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞)
        - Clarity (—è—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è)
        
        –í–µ—Ä–Ω–∏ –æ—Ü–µ–Ω–∫—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
        """
        
        evaluation = ollama.generate(
            model='deepseek-r1:8b',
            prompt=evaluation_prompt,
            options={'temperature': 0.1}
        )
        
        try:
            import json
            scores = json.loads(evaluation['response'])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –æ—Ü–µ–Ω–∫—É
            total_score = sum(scores.values()) / len(scores)
            scores['overall'] = round(total_score, 2)
            
            return scores
        except:
            return {metric: 5 for metric in self.metrics + ['overall']}
    
    def provide_feedback(self, question: str, response: str, scores: dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"""
        
        feedback_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å:
        
        –í–æ–ø—Ä–æ—Å: {question}
        –û—Ç–≤–µ—Ç: {response}
        –û—Ü–µ–Ω–∫–∏: {scores}
        
        –ü—Ä–µ–¥–ª–æ–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞.
        –ë—É–¥—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.
        """
        
        feedback = ollama.generate(
            model='qwen2.5:7b',
            prompt=feedback_prompt,
            options={'temperature': 0.7}
        )
        
        return feedback['response']
    
    def track_improvement(self, evaluations: List[dict]) -> dict:
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å —Ç–µ—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏"""
        if not evaluations:
            return {}
        
        improvement = {}
        for metric in self.metrics + ['overall']:
            first_score = evaluations[0].get(metric, 5)
            last_score = evaluations[-1].get(metric, 5)
            improvement[metric] = {
                'start': first_score,
                'current': last_score,
                'improvement': last_score - first_score,
                'trend': 'positive' if last_score > first_score else 'negative'
            }
        
        return improvement

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
evaluator = QualityEvaluator()

question = "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?"
response = "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö."

scores = evaluator.evaluate_response(question, response)
feedback = evaluator.provide_feedback(question, response, scores)

print("üìä –û—Ü–µ–Ω–∫–∏:", scores)
print("üí° –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å:", feedback)
```